{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REACT AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "tavily_api_key = os.getenv('TAVILY_API_KEY')\n",
    "model_id = os.getenv('MODEL_ID')\n",
    "aws_region = os.getenv('AWS_REGION')\n",
    "bedrock_kb_id = os.getenv('BEDROCK_KB_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "llm = ChatBedrock(max_tokens=8192, model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def retriever_tool(query):\n",
    "    \"\"\"Query the knowledge base for information related to Agents and Agentic workflow\n",
    "    \n",
    "    Args:\n",
    "        query: The query string to search for\n",
    "    \"\"\"\n",
    "    bedrock_agent = boto3.client('bedrock-agent-runtime', region_name = aws_region)\n",
    "    print(\"QUERYING KB\")\n",
    "    response = bedrock_agent.retrieve_and_generate(\n",
    "        input={\n",
    "            \"text\": query  # Your query text goes here\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": bedrock_kb_id,\n",
    "                \"modelArn\": model_id,\n",
    "                \"retrievalConfiguration\": {\n",
    "                    \"vectorSearchConfiguration\": {\n",
    "                        \"numberOfResults\": 5\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    kb_results = response['output']['text']\n",
    "    return kb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def blogger_tool(context, language):\n",
    "    \"\"\"Ue this tool to write a well formatted blog with a blog title\n",
    "    \n",
    "    Args:\n",
    "        context: The context for the blog\n",
    "        language: Language to write the blog; choose 'English' as default if not mentioned by user.\n",
    "    \"\"\"\n",
    "    print(\"WRITING BLOG\")\n",
    "    prompt = f\"\"\" Your job is to create a blog title and a detailed multi-paragraph blog with sections from this content: {context} \\n\\n The Blog has to be written in Language : {language}\"\"\"\n",
    "\n",
    "    final_answer = llm.invoke(prompt)           \n",
    "    return final_answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def translate_tool(blog_content, language):\n",
    "    \"\"\"Use this tool to translate the blog_content from english to any other language. The context here is the complete blog written in a different language to the one user asked for.\n",
    "    \n",
    "    Args:\n",
    "        blog_content: The blog to be translated\n",
    "        language: The language to translate to\n",
    "    \"\"\"\n",
    "    print(\"BLOG TRANSLATION\")\n",
    "    prompt = f\"\"\" Your job is to translate the blog:\\n {blog_content}. \\n\\n Translate to {language} from source language. Make sure to be comprehensive when translating; cover complete context.\"\"\"\n",
    "\n",
    "    final_answer = llm.invoke(prompt)           \n",
    "    return final_answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, blogger_tool, translate_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build the agent (Prebuilt Langgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "graph = create_react_agent(\n",
    "    llm, tools=tools, \n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stream the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    \"\"\"A utility to pretty print the stream.\"\"\"\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "config = {\"configurable\": {\"thread_id\": \"42\"}}\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(\"what a blog about the types of agents\")]}\n",
    "print_stream(graph.stream(inputs, config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(\"Can you translate it to deutsch?\")]}\n",
    "print_stream(graph.stream(inputs, config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build the agent (Custom with Post-processing Node) (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Dict,\n",
    "    Tuple\n",
    ")\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "    # add_messages is a reducer\n",
    "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    formatted_blog: Dict\n",
    "    \n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Define our tool node\n",
    "def tool_node(state: AgentState):\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "# Define the node that calls the model\n",
    "def call_model(\n",
    "    state: AgentState,\n",
    "    config: RunnableConfig,\n",
    "):\n",
    "    # this is similar to customizing the create_react_agent with 'prompt' parameter, but is more flexible\n",
    "    system_prompt = SystemMessage(\n",
    "        \"You are a helpful AI assistant who can write a blog using knowledge retreived and also translate it when required.\" \n",
    "        \"\\n Please respond to the users query to the best of your ability using tools made available to you.\"\n",
    "        \"\\n translate_tool should be called only to translate the overall blog and not the conversations. Once translate is called, that will be the last step for that converstaion.\"\n",
    "    )\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    response = llm_with_tools.invoke([system_prompt] + state[\"messages\"], config)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage, HumanMessage, SystemMessage,AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class FormattedResponse(BaseModel):\n",
    "    \"\"\"Respond to the user in this format.\"\"\"\n",
    "    Blog_Title: str = Field(description=\"title of the blog\")\n",
    "    Blog_Content: str = Field(description=\"content of the blog\")\n",
    "    Blog_language: str = Field(description=\"language of the blog\")\n",
    "\n",
    "def formatter_node(state):\n",
    "    \"\"\"\n",
    "    Post-processing node that formats the final blog output using structured LLM parsing.\n",
    "    Only uses tool outputs from blogger_tool or translate_tool after the most recent human message.\n",
    "    \"\"\"\n",
    "    print(\"FORMATTING BLOG WITH STRUCTURED PARSER\")\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Step 1: Find the index of the most recent HumanMessage\n",
    "    last_human_index = -1\n",
    "    human_msg = None\n",
    "    for i in reversed(range(len(messages))):\n",
    "        if isinstance(messages[i], HumanMessage):\n",
    "            last_human_index = i\n",
    "            human_msg = messages[i]\n",
    "            break\n",
    "\n",
    "    if last_human_index == -1:\n",
    "        print(\"No recent HumanMessage found. Skipping formatting.\")\n",
    "        return {}\n",
    "\n",
    "    # Step 2: Gather ToolMessages from blogger_tool or translate_tool after the last HumanMessage\n",
    "    relevant_content = []\n",
    "    for msg in messages[last_human_index + 1:]:\n",
    "        if isinstance(msg, ToolMessage) and msg.name in [\"blogger_tool\", \"translate_tool\"]:\n",
    "            relevant_content.append(msg.content)\n",
    "\n",
    "    if not relevant_content:\n",
    "        error_msg = \"No recent blogger/translate tool outputs found.\"\n",
    "        print(error_msg)\n",
    "        return {\"formatted_blog\":{\"error\":error_msg}}\n",
    "\n",
    "    combined_blog_text = \"\\n\\n\".join(relevant_content)\n",
    "\n",
    "    # Step 3: Format with structured output\n",
    "    format_prompt = SystemMessage(\n",
    "        content=(\n",
    "            \"You will be given a series of tool outputs from a blogging agent as input. The Content can be in any language; depending on whethere translation was performed.\"\n",
    "            \"Extract or Derive and format it into a JSON object with the following fields:\\n\"\n",
    "            \"\\t- Blog_Title: The title of the blog. Create a title if unable to extract\\n\"\n",
    "            \"\\t- Blog_Content: The content in MD format; Add Headers, Subheaders, Highlighting, Bullets, Numbering wherever required. Use the final language in which blog was written when generating blog_content\\n\"\n",
    "            \"\\t- Blog_language: The language used (detect it)\\n\\n\"\n",
    "            \"Respond only with the Structured format.\\n\"\n",
    "        )\n",
    "    )\n",
    "    structured_llm = llm.with_structured_output(FormattedResponse)\n",
    "    final_prompt = [format_prompt, HumanMessage(content=f\"Here is the context of the blog for you to provide me with a structured output:\\n{combined_blog_text}\")]\n",
    "    result = structured_llm.invoke(final_prompt)\n",
    "    print(result)\n",
    "    return {\n",
    "        \"messages\":AIMessage(\"Formatting complete, graph state updated - `formatted_blog`\",metadata=result.model_dump()),\n",
    "        \"formatted_blog\": result.model_dump()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "# Define the additional node that will be used to format the work after agent finished interactions with tools\n",
    "workflow.add_node(\"format_blog\",formatter_node)\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"tools\",\n",
    "        # Otherwise we finish. (Calling Format_blog is considered last step of agent)\n",
    "        \"end\": \"format_blog\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "# We now add a normal edge from `format_blog` to `END`.\n",
    "# This means that after `format_blog` is called, graph workflow is complete.\n",
    "workflow.add_edge(\"format_blog\", END)\n",
    "\n",
    "# Now we can compile and visualize our graph\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    \"\"\"A utility to pretty print the stream.\"\"\"\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "config = {\"configurable\": {\"thread_id\": \"41\"}}\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(\"what a blog about the types of agents\")]}\n",
    "print_stream(graph.stream(inputs, config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.get_state(config).values.get(\"formatted_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(\"translate it to french\")]}\n",
    "print_stream(graph.stream(inputs, config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(config).values.get(\"formatted_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(\"traducir el texto al espa√±ol\")]}\n",
    "print_stream(graph.stream(inputs, config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(config).values.get(\"formatted_blog\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
